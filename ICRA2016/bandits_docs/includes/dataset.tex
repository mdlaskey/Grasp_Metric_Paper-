\section{Dexterity Network}
\seclabel{dexnet}

The Dexterity Network (Dex-Net) 1.0 dataset is a growing set that currently includes over 10,000 unique 3D object models annotated with 2.5 million parallel-jaw grasps.

\subsection{Data}
Dex-Net 1.0 contains 13,252 3D mesh models: 8,987 from the SHREC 2014 challenge dataset~\cite{li2015comparison}, 2,539 from ModelNet40~\cite{wu20153d}, 1,371 from 3DNet~\cite{wohlkinger20123dnet}, 129 from the KIT object database$^*$~\cite{kasper2012kit}, 120 from BigBIRD$^*$~\cite{singh2014bigbird}, 80 from the Yale-CMU-Berkeley dataset$^*$~\cite{calli2015benchmarking}, and 26 from the Amazon Picking Challenge$^*$ scans ($^*$ indicates laser-scanner data).
We preprocess each mesh by removing unreferenced vertices, computing a reference frame with Principal Component Analysis (PCA) on the mesh vertices, setting the mesh center of mass $\bz$ to the center of the mesh bounding box, and rescaling the synthetic meshes to fit the smallest dimension of the bounding box within $w = 0.1m$.
To resolve orientation ambiguity in the reference frame, we orient the positive $z$-axis toward the side of the $xy$ plane with more vertices. 
We also convert each mesh to an SDF using SDFGen~\cite{sdfgen}.

\subsection{Grasp Sampling}
\seclabel{grasp-sampling}
Each 3D object $\mO_i$ in Dex-Net is labelled with up to 250 parallel-jaw grasps and their $P_F$.
We generate $K$ grasps for each object using a modification of the 2D algorithm presented in Smith et al.~\cite{smith1999computing} to concentrate samples on grasps that are antipodal~\cite{mahler2015gp}.
To sample a single grasp, we generate a contact point $\bc_1$ by sampling uniformly from the object surface $\mS$, sample a direction $\bv \in \bS^2$ uniformly at random from the friction cone, and find an antipodal contact $\bc_2$ on the line $\bc_1 + t\bv$ where $t \geq 0$.
We add the grasp $\bg_{i,k} = (0.5(\bc_1 + \bc_2), \bv)$ to the candidate set if the contacts are antipodal~\cite{mahler2015gp}.
We evaluated $P_F(\bg_{i,k})$ using Monte-Carlo integration~\cite{kehoe2012toward, weisz2012pose} by sampling the object pose, gripper pose, and friction random variables $N=500$ times and recording $Z_{i,k}$, the number of samples for which $\bg_{i,k}$ achieved force closure ($F=1$).


%We generate a set of grasps for each object in the network using the method of \secref{candidates} and evaluate the probability of force closure for each grasp using brute-force Monte Carlo integration as a benchmark~\cite{kehoe2012toward}.
%As the number of models is quite large, we distribute the grasp labelling for each object across virtual machines in Google Compute Engine and aggregate the results at the end.

\subsection{Grasp Differential Depthmap Features}
\seclabel{grasp-similarity}
To measure grasp similarity, we embed each grasp $\bg = (\bx ,\bv)$ on object $\mO$ in Dex-Net in a feature space based on a 2D map of the  local surface orientation at the contacts, inspired by grasp heightmaps~\cite{herzog2014learning, kappler2015leveraging}.
We generate a depthmap $\bd_{i}$ for contact $\bc_i$ by orthogonally projecting the local object surface onto an $m \times m$ grid centered at $\bc_i$ and oriented along the line to the object center of mass, $\ba_i = \bz - \bc_i$.
Since $F$ only depends on $\bc_i$ and its surface normal, rotations of $\bd_i$ about $\ba_i$ correspond to grasps of the equivalent quality.
We therefore make each $\bd_i$ rotation-invariant by orienting its axes along the eigenvectors of a weighted covariance matrix of the 3D surface points that generate $\bd_i$ as described in~\cite{salti2014shot}.
\figref{local-feature-model} illustrates local surface patches extracted by this procedure.
We finally take the $x$- and $y$-image gradients of $\bd_i$ to form differential depthmaps $\nabla_x \bd_{i}$ and $\nabla_y \bd_{i}$, motivated by the dependence of $F$ on surface normals~\cite{pokorny2013c}, 
The feature vector for each grasp-object pair  in Dex-Net is $\eta(\bg, \mO) = (\nabla_x \bd_{1}, \nabla_y \bd_{1}, \nabla_x \bd_{2}, \nabla_x \bd_{2})$.

\begin{figure}[t!]
\centering
\includegraphics[scale=0.30]{figures/illustrations/local_feature_model.eps}
\caption{Illustration of three local surface depthmaps extracted on a teapot. Each depthmap is ``rendered" along the grasp axis $\bv_i$ at contact $\bc_i$ and oriented by the directions of maximum variation in the depthmap.  We use gradients of the depthmaps for similiarity between grasps in Dex-Net.}
\figlabel{local-feature-model}
\vspace*{-15pt}
\end{figure}

\section{Deep Learning for Object Similarity}
\seclabel{object-similarity}
We use Multi-View Convolutional Neural Networks (MV-CNNs)~\cite{su2015multi} to efficiently index prior 3D object and grasp data from Dex-Net by embedding each object in a vector space where distance represents object similarity, as shown in \figref{global-feature-model}.
We first render every object on a white background in a total of $N_c = 50$ virtual camera views oriented toward the object center and spaced on a grid of angle increments $\delta_{\theta} = \frac{2 \pi}{5}$ and $\delta_{\phi} = \frac{2 \pi}{5}$ on a viewing sphere with radii $r = R, 2R$, where $R$ is the maximum dimension of the object bounding box.
Then we train a CNN with the architecture of AlexNet~\cite{krizhevsky2012imagenet} to predict the 3D object class label for the rendered images on a training set of models. 
We initialize the weights of the network with the weights learned on ImageNet by Krizhevsky et al.~\cite{krizhevsky2012imagenet} and optimize using Stochastic Gradient Descent (SGD). 
Next, we pass each of the $N_c$ views of each object through the optimized CNN and max-pool the output of the fc7 layer, the highest layer of the network before the class label prediction. 
Finally, we use Principal Component Analysis (PCA) to reduce the max-pooled output from 4,096 dimensions to a 100 dimensional feature vector $\psi(\mO)$.

Given the MV-CNN object representation, we measure the dissimilarity between two objects $\mO_i$ and $\mO_j$ by the Euclidean distance $\| \psi(\mO_i) - \psi(\mO_j) \|_2$.
For efficient lookups of similar objects, Dex-Net contains a KD-Tree nearest neighbor query structure with the feature vectors of all prior objects.
In our implementation, we trained the MV-CNN using the Caffe library~\cite{jia2014caffe} on rendered images from a training set of approximately $6,000$ 3D models sampled from the SHREC 2014~\cite{li2015comparison} portion of Dex-Net, which has 171 unique categories, for 500,000 iterations of SGD.
%The final PCA step captured approximately 85\% of the variance of the vectors.
%The image classification training step had a test accuracy of 76\%.
To validate the implementation, we tested on the SHREC 2014 challenge dataset and achieved a 1-NN accuracy of 86.7\%, compared to 86.8\% achieved by the winner of SHREC 2014~\cite{li2015comparison}.

\begin{figure}[t!]
\centering
\includegraphics[scale=0.275]{figures/illustrations/cnn_model.eps}
\caption{Illustration of our Multi-View Convolutional Neural Network (MV-CNN) deep learning method for embedding 3D object models in a Euclidean vector space to compute global shape similarity. We pass a set of 50 virtually rendered camera viewpoints discretized around a sphere through a deep Convolutional Neural Network (CNN) with the AlexNet~\cite{krizhevsky2012imagenet} architecture. Finally, we take the maximum fc7 response across each of the 50 views for each dimension and run PCA to reduce dimensionality.}
\figlabel{global-feature-model}
\vspace*{-15pt}
\end{figure}

