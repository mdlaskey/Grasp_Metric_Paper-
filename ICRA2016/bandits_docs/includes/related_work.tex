\section{Related Work}
\seclabel{related}

%For a survey of the substantial literature on grasping, see Prattichizzo and Trinkle~\cite{prattichizzo2008grasping}.
Considerable research on grasp planning has focused on finding grasps by maximizing an analytic grasp quality metric based on a wrench space (WS), such as the grasp WS~\cite{ferrari1992, ciocarlie2009}, object WS~\cite{liu2015fast}, or task WS~\cite{kruger2011partial}.
The need to recompute WS metrics for every new 3D object and grasp motivated research on grasp planning by transferring grasps from similar 3D objects in a database of prior knowledge~\cite{li2005shape}.
Goldfeder et al.~\cite{goldfeder2009columbia} developed the Columbia grasp database, a dataset of 1,814 distinct models and over 200,000 force closure grasps generated using the Eigengrasp planner in GraspIt!~\cite{ciocarlie2009}.
The authors later used synthetic partial depth maps of objects in the database to match robot sensor data to precomputed grasps using the Iterated Closest Point algorithm~\cite{goldfeder2011data}.
Research has also studied transferring grasps from objects of the same category by warping contacts betwen corresponding points on a shape surface and using local alignment and contact interpolation~\cite{stouraitis2015functional} or by interpolating grasps and shapes over a vector space representation called a Grasp Moduli Space~\cite{pokorny2013grasp}.

Recent research has also studied labelling grasps in a database with metrics that are robust to imprecision in perception and control~\cite{brook2011collaborative}.
%, due to the fragility of analytic WS grasp metrics~\cite{balasubramanian2012physical, diankov2010automated, weisz2012pose}.
Weisz et al.~\cite{weisz2012pose} found that grasps ranked by probability of force closure subject to perturbations of object pose in simulation were empirically more successful on a physical robot than grasps planned using deterministic WS metrics. 
Similarly, Kim et al.~\cite{kim2012physically} planned grasps using the expected Ferrari-Canny quality~\cite{ferrari1992} under uncertainty in object pose using dynamic simulation to determine contacts, and found that the robust metric was more highly correlated with physical grasp success.
Kehoe et al.~\cite{kehoe2013cloud} created a Cloud-based system to transfer grasps evaluated by probability of force closure on objects in a database to a physical robot by indexing the objects with the Google Goggles object recognition engine.
The authors found that the full system of object recognition, pose estimation, and grasp execution successfully grasped objects on a table in 80\% of trials.

Another line of research has focused on synthesizing grasps using statistical models learned from a database of objects annotated with grasps decided by humans or physical execution~\cite{bohg2014data}.
%Saxena et al.~\cite{jiang2011efficient, saxena2008robotic} used a logistic regression classified to predict grasp affordances in images from human annotated training data.
Herzog et al.~\cite{herzog2012template, herzog2014learning} extracted heightmaps of local object curvature from human demonstrated grasps to construct a library of heightmap templates, and matched new sensor data to templates to select grasps similar to demonstrations.
Detry et al.~\cite{detry2012generalizing} created a low-dimensional representation of object parts and cluster object parts that are grasped similarly to form a library of prototypical graspable point clouds, and showed that the representation could be transferred to real sensor data~\cite{detry2013learning}.
Lenz et al.~\cite{lenz2015deep} used deep learning to detect bounding boxes for parallel-jaw grasps in color and depth images based on a dataset of thousands of 1035 images annotated with parallel-jaw grasps by humans.
Kappler et al.~\cite{kappler2015leveraging} created a database of over 700 object instances each labelled with 500 Barrett hand grasps and their associated quality from human annotations and the results of simulations with the ODE physics engine.
The authors trained a deep neural network to predict grasp quality from heightmaps of the local object surface rendered along the surface normal.
In comparison, we learn a model to predict a belief distribution on the probability of force closure for a parallel-jaw grasp on a new object based on similarity to prior 3D objects and grasps, and we actively choose the next grasp to evaluate using Multi-Armed Bandits.

Our work is also closely related to research on actively selecting grasps for building a statistical model of grasp quality from fewer examples~\cite{salganicoff1996active}.
%Several works have searched for successful grasps using belief space planning to minimze uncertainty~\cite{hsiao2007grasping, kahnactive, fischinger2015learning}, but without use of an explicit grasp quality metric to guide the search.
Kehoe et al.~\cite{kehoe2012estimating} proposed iterative pruning, an algorithm for evaluating the probability of force closure for a set of candidate grasps while discarding grasps known to have poor quality.
Detry et al.~\cite{detry2011learning} estimated a full continuous density function over grasp poses using kernel density estimation and adaptively acquired samples by pruning unsuccessful grasps.
Kroemer et al.~\cite{kroemer2010combining} developed a reinforcement learning approach to grasp selection based on seeding hypotheses via imitation learning and Gaussian process upper-confidence bounds for active grasp acquisition.
%Boularias et al.~\cite{boularias2014efficient, boularias2015learning} used a Gaussian process Bayesian Optimization model for selecting grasps on cluttered piles of rocks.
%Salaganicoff et al.~\cite{salganicoff1996active} used active learning to decide the next grasp to execute on a physical robot while learning a predicitve model of empirical success from range sensors.
Montesano and Lopes~\cite{montesano2012active} used Continuous Correlated Beta Processes~\cite{goetschalckx2011continuous} to actively acquire grasp executions on a physical robot, measuring correlations from the responses to a bank of image filters designed to detect grasp affordances such as edges.
Recently, Laskey et al.~\cite{laskey2015bandits} used Multi-Armed Bandit (MAB) algorithms to reduce the number of samples needed to identify grasps with high probability of force closure under uncertainty in object shape, pose, and friction in 2D.
%MAB algorithms trade off gaining information about grasps that have been sampled fewer times with exploiting the grasp with the highest estimated quality given the past samples.
In this work we extend the model of Laskey et al.~\cite{laskey2015bandits} to 3D and to utilize similarity between grasps across objects from Dex-Net to further accelerate convergence to a grasp with high probability of force closure.

% DEEP LEARNING
To use the prior information contained in our database on a new model, we also draw on past research on 3D model similarity.
One can separate the methods to compare 3D shapes in roughly two categories.
On one side, some methods encode the geometric characteristics of the shape, such as its curvature or characteristics of the harmonic functions on the shape~\cite{horn1984extended, kazhdan2003rotation, bronstein2011shape}. 
hese methods mainly rely on the use of hand-design descriptors, with the notable exception of~\cite{wu20153d} which learn a CNN on the voxel representation of the shape.
On the other hand, view-based descriptors rely on the description of rendered views of a 3D model~\cite{murase1995visual,chen2003visual}.
One of the key difficulty of these methods is to aggregate or compare views from different objects, which may be oriented consistently.
The recent work of Su et al.~\cite{su2015multi} addresses this issue by using CNN trained for ImageNet classification as descriptors for the different views, aggregating them with second CNN which learns the invariance to orientation.
Using this method, the authors improve state of the art classification results on ModelNet40 by a large margin.
In a different set up, a simpler average pooling of the features from different views was proposed in~\cite{aubry2015understanding}.
We use a similar strategy by max-pooling the features, which we found to perform better than average pooling.

